{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Get the LLM responses for the correction request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first look at the texts to be corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cualquier cosa, pues solo se hizo circular en hoja suelta: pero lo haremos conocer próximamente, AREQUIPA MARZO 5 DB 1884. pues tenemos el propósito de no omitir : esfuerzo hasta conseguirla. Mientras tanto, como reminiscencias que de ella se conservan en la memoria, insertamos uno de sus fragmentos, que poco mas ó menos dice a \"Casas sin techo, Rio sin agua, Arboles sin hojas, Muchacho malcriado ...... Todo esto era el Perú A la muerte del General Castilla.“ Si hay alguna alteracion en esta parte, el autor por interes propio debe exhibir la Oda para rectificarla, previniendose que aun cuando parezca exagerado, este fragmento es de los menos malos. Y a proposito. ¿ Que daño pudo haberle hecho a este pedante el ilustre General Castilia, que con tanto desden miraba a los pequeños, para que intentase escarnecer y poner en rídiculo su respetada memoria? Nos esplicamos el motivo del encono que abriga el Redactor de \"El Peru\" para el país de su nacimiento, pero .... la saña que revela no solo la hace estensiva a todo el Perú, sino que la lleva hasta Francia, y escoje á una de sus mas esclarecidas inteligencias, para presentarla con todas las sombras de la monstruosidad, traduciendo a Victor-Hugo, a la manera de aquel mal pintor que para vengarse de Nada habria de un enemigo suyo ,...... lo retrató. estraño en que si Valdes tuviera motivo de odio para Italia ó Alemania, por ejemplo, hicese variaciones sobre las mejores armonias de Rosini ó de Mozart. Mas, dejando à un lado esta digresion, nos preguntamos: ¿ que objeto se propuso el Hacedor Supremo al crear á este ser incalificable? ¡Inescrutables designios de la Providencia! Para formar el contraste ha sido necesario colocar á no al lado de la paloma, Is víbora; de las tinieblas, la luz; de la virtud, la perversidad; y del buen nombre de Arequipa, Valdes. Pero el lector preguntará que objeto tiene esta publicacion .- El siguiente: que por grande que haya sido el desprecio con que se ha mirado las infolices producciones del redactor de \"El Perú\" cuando no se ha cebado en la honra de personas cuya vida no presenta mancha, ha llegado el caso de que su procacidad y calumniosas aseveraciones, desmentidas por élá cada paso, sean reprimidas debidamente, a fin de que fuera del lugar sea desvanecido el mal con cepto que el Sr. Valdes procura se tenga de Arequipa; pues como dice el distinguido crítico español D . Juan M. Villergas; ...... \" Hay otra clase mas, contraria a la civilizacion .... y es la de los malos escritores, la de los literatos que hacen detestables traducciones, intolerables plagios, disparates groseros; en una pala\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"../data/cleaned-latam-xix.parquet\")\n",
    "print(df.loc[0, \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the LLM API Client and request parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the LLM is GPT in it's **GPT 4o** version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_price(model, in_tk, out_tk):\n",
    "    prices_per_1k = {\n",
    "        \"gpt-4o\": {\"input\": 0.005, \"output\": 0.015},\n",
    "        \"gpt-4o-mini\": {\"input\": 0.00015, \"output\": 0.0006},\n",
    "    }\n",
    "\n",
    "    if model not in prices_per_1k:\n",
    "        raise ValueError(f\"Model {model} not found in the pricing list\")\n",
    "\n",
    "    input_cost = (in_tk / 1000) * prices_per_1k[model][\"input\"]\n",
    "    output_cost = (out_tk / 1000) * prices_per_1k[model][\"output\"]\n",
    "\n",
    "    return input_cost + output_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('./.env')\n",
    "client = AzureOpenAI(\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY')\n",
    ")\n",
    "engine = os.getenv(\"AZURE_OPENAI_IMPLEMENTATION\")\n",
    "model = os.getenv(\"MODEL\")\n",
    "encoder = tiktoken.encoding_for_model(model)\n",
    "count_tk = lambda t: len(encoder.encode(t))\n",
    "MAX_RETRIES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request(prompt, max_tokens=4096, temperature=0, try_count=0):\n",
    "    \"\"\"Request a completion from the OpenAI API.\n",
    "    :param prompt: The prompt to send to the API\n",
    "    :param max_tokens: The maximum number of tokens in the output\n",
    "    :param temperature: The degree of randomness in the output\n",
    "    :param try_count: Retry control parameter\n",
    "    :return response: The response from the API\n",
    "    :return usage: The count of token usage from the API { \"input\", \"output\" }\n",
    "    \"\"\"\n",
    "    expected_tokens = count_tk(prompt)\n",
    "    if expected_tokens > 4000:\n",
    "        return \"\", {\"input\": 0, \"output\": 0}, f\"length - INPUT too long ({expected_tokens} tokens)\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=engine,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "        finish_reason = response.choices[0].finish_reason\n",
    "        if finish_reason == \"content_filter\":\n",
    "            rsp = \"\"\n",
    "            content_filter = response.choices[0].content_filter_results\n",
    "            prompt_filter = response.prompt_filter_results[0]['content_filter_results']\n",
    "            finish_reason += f\" - {get_filter_flags(content_filter, prompt_filter)}\"\n",
    "        else:\n",
    "            rsp = response.choices[0].message.content\n",
    "        return rsp, { \"input\": response.usage.prompt_tokens, \"output\": response.usage.completion_tokens }, finish_reason\n",
    "    except Exception as e:\n",
    "        response = \"\"\n",
    "        try: usage = { \"input\": response.usage.prompt_tokens, \"output\": response.usage.completion_tokens }\n",
    "        except: usage = { \"input\": 0, \"output\": 0 }\n",
    "        if \"content management policy\" in f\"{e}\":\n",
    "            #finish_reason = \"content_filter - content_management_policy\"\n",
    "            finish_reason = f\"ERROR [{type(e).__name__}]: {e}\"\n",
    "        else:\n",
    "            if \"Max retries exceeded with url\" in f\"{e}\":\n",
    "                if try_count >= MAX_RETRIES:\n",
    "                    return response, usage, f\"RETRYING, but reached MAX_RETRIES ({MAX_RETRIES})\"\n",
    "                print(f\"RETRYING ({try_count})...\")\n",
    "                time.sleep(60)\n",
    "                return request(prompt, max_tokens, temperature, try_count+1) # retry after 60 seconds\n",
    "            finish_reason = f\"ERROR [{type(e).__name__}]: {e}\"\n",
    "        return response, usage, finish_reason\n",
    "\n",
    "def get_filter_flags(content_filter, prompt_filter):\n",
    "    return ', '.join(\n",
    "        [f\"content.{k}\" for k, v in content_filter.items() if v != {'filtered': False, 'severity': 'safe'}] +\n",
    "        [f\"prompt.{k}\" for k, v in prompt_filter.items() if (k == 'jailbreak' and v != {'filtered': False, 'detected': False}) or (k != 'jailbreak' and v != {'filtered': False, 'severity': 'safe'})]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the prompt to send with the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_start = 'Dado el texto del siglo xix entre ```, retorna únicamente el texto corrigiendo los errores ortográficos sin cambiar la gramática. No corrijas ortografía de nombres:\\n```\\n'\n",
    "# text\n",
    "prompt_end = '\\n```'\n",
    "gen_prompt = lambda text: f\"{prompt_start}{text}{prompt_end}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recover the information from the last execution and avoid the lost of data if an error occurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSES_FILE = \"./responsesLatam.json\"\n",
    "\n",
    "r = {\"data\":[], \"checkpoint\": 0, \"input_tokens\": 0, \"output_tokens\": 0, \"fail_input_tokens\": 0, \"fail_output_tokens\": 0, \"total_price\": 0, \"price_per_req\": 0}\n",
    "if os.path.exists(RESPONSES_FILE):\n",
    "    with open(RESPONSES_FILE, \"r\") as f:\n",
    "        r = json.load(f)\n",
    "else:\n",
    "    with open(RESPONSES_FILE, \"w\") as f:\n",
    "        f.write(json.dumps(r, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 51860/64077 (80.93%)\n"
     ]
    }
   ],
   "source": [
    "assert r['checkpoint'] == len(r['data']), \"Checkpoint does not match with corrected texts\"\n",
    "print(f\"Done {r['checkpoint']}/{len(df)} ({100*r['checkpoint']/len(df):.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Send the requests to the API and store them periodically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED (51870)\n",
      "SAVED (51880)\n"
     ]
    }
   ],
   "source": [
    "for text in df.loc[r['checkpoint']:, \"text\"]:\n",
    "    prompt = gen_prompt(text)\n",
    "    #print(f\"------------------------------- {r['checkpoint']+1} -----------------------------------\")\n",
    "    #print(prompt)\n",
    "    response, usage, finish_reason = request(prompt)\n",
    "    #print(response if response else f\"ERROR: {finish_reason}\")\n",
    "    if finish_reason in ['stop', 'length']:\n",
    "        r[\"input_tokens\"] += usage[\"input\"]\n",
    "        r[\"output_tokens\"] += usage[\"output\"]\n",
    "    else:\n",
    "        r[\"fail_input_tokens\"] += usage[\"input\"]\n",
    "        r[\"fail_output_tokens\"] += usage[\"output\"]\n",
    "\n",
    "    r[\"data\"].append({\n",
    "        \"text\": text,\n",
    "        \"resp\": response,\n",
    "        \"finish_reason\": finish_reason\n",
    "    })\n",
    "\n",
    "    r[\"checkpoint\"] += 1\n",
    "\n",
    "    if r['checkpoint'] % 10 == 0:\n",
    "        price = compute_price(model, r['input_tokens']+r[\"fail_input_tokens\"], r['output_tokens']+r[\"fail_output_tokens\"])\n",
    "        r['total_price'] = round(price, 4)\n",
    "        r['price_per_req'] = round(price / r['checkpoint'], 6)\n",
    "        with open(RESPONSES_FILE, \"w\") as f:\n",
    "            f.write(json.dumps(r, indent=4))\n",
    "        print(f\"SAVED ({r['checkpoint']})\")\n",
    "\n",
    "with open(RESPONSES_FILE, \"w\") as f:\n",
    "    f.write(json.dumps(r, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be empty responses due to errors when sending the request. Some of them are due to connection issues, and others due to the OpenAI's content management policy. In this case, the request, can be run manually when there are only a few cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The following requests must be repeated manually (due to OpenAI's content filter or an error):\\n\")\n",
    "for i,e in enumerate(r[\"data\"]):\n",
    "    if e[\"finish_reason\"].startswith(\"content_filter\") or e[\"finish_reason\"].startswith(\"ERROR\"):\n",
    "        print(f\"------------------------------- {i} -----------------------------------\")\n",
    "        print(gen_prompt(e['text']))\n",
    "\n",
    "# put the responses in the dictionary and run the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manually_req_responses = {\n",
    "    # ...\n",
    "}\n",
    "\n",
    "for i,e in enumerate(r[\"data\"]):\n",
    "    if e[\"finish_reason\"] == \"content_filter\" or e[\"finish_reason\"].startswith(\"ERROR\"):\n",
    "        print(f\"------------------------------- {i} -----------------------------------\")\n",
    "        print(r[\"data\"][i]['text'])\n",
    "        if i in manually_req_responses:\n",
    "            r[\"data\"][i][\"resp\"] = manually_req_responses[i]\n",
    "            r[\"data\"][i][\"finish_reason\"] = \"stop\"\n",
    "            print(r[\"data\"][i][\"resp\"])\n",
    "            print(f\"Index {i} OK\")\n",
    "        else:\n",
    "            print(f\"Index {i} NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RESPONSES_FILE, \"w\") as f:\n",
    "    f.write(json.dumps(r, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
